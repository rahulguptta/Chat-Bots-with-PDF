# -*- coding: utf-8 -*-
"""Chatbots_Local_Streamlit_using_PDFs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v2WynhythJM3HxV7jH-j3UH3ysCwL58j
"""

# Setting for local and stream lit
running_local = False # plateform identifier
STATE = {} # vector DB store
specific_key = False

# Installing required packages
if running_local:
  subprocess.run(
        [sys.executable, "-m", "pip", "install", "-r", "requirements.txt", "-q"],
        check=True
    )

# Importing required libraries
import streamlit as st
import os
import time
from langchain_groq import ChatGroq
## from pdf to vectordb
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
## chain and prompts
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains import create_retrieval_chain

# keys
if running_local:
  from dotenv import load_dotenv
  load_dotenv("/content/.env", override = True)
  api_key = os.getenv("GROQ_API_KEY")
  #print(f"api_key: {api_key}")
  HF_TOKEN = os.getenv("HF_TOKEN")
  LANGCHAIN_API_KEY = os.getenv("LANGCHAIN_API_KEY")

else:
  api_key = st.secrets["GROQ_API_KEY"]
  HF_TOKEN = st.secrets["HF_TOKEN"]

LANGCHAIN_TRACING_V2 = "true"
LANGCHAIN_PROJECT ="ChatBots_Local_Streamlit_using_PDFs"

#
prompt = ChatPromptTemplate.from_template(
    """
    Answer the question based only on the provided context.
    Please provide the most accurate response based on the question.

    <context>
    {context}
    </context>

    Question: {input}
    """
)

# model and parameters
if running_local:
  engine = "llama-3.1-8b-instant"
  temperature = 0.7
  max_tokens = 150
else:
  st.title("ChatBots Using PDFs")
  st.write("Fire a question")
  st.sidebar.title("Settings")
  engine = st.sidebar.selectbox(
      label = "Select LLM Model",
      options = ["llama-3.1-8b-instant", "qwen/qwen3-72B"],
      index = 0
  )
  specific_key = st.sidebar.text_input(label = "Specific_key(optional)", type = "password")
  temperature = st.sidebar.slider(label = "Temperature",
                                  min_value = 0.0,
                                  max_value = 1.0,
                                  value = 0.7)
  max_tokens = st.sidebar.slider(label = "Max Tokens",
                                 min_value = 50,
                                 max_value = 300,
                                 value = 150)

# Functions for storing vector db
def create_vector_embeddings(running_local = False):
  store = STATE if running_local else st.session_state
  pdf_dir = "/content/sample_data/research_papers" if running_local else "research_papers"

  @st.cache_resource(show_spinner=False) #Streamlit caching decorator used to cache expensive resources so they are created only once per app session, without showing the loading spinner
  def get_embedder():
    emb = HuggingFaceEmbeddings(model_name = "sentence-transformers/all-MiniLM-L6-v2")
    test_vec = emb.embed_query("hello world")
    assert isinstance(test_vec, list) and len(test_vec)>0, "Embedding model returned empty vector."
    st.write(f"Embedding dim: {len(test_vec)}")
    return emb

  embeddings = get_embedder()

  loader = PyPDFDirectoryLoader(pdf_dir)
  docs = loader.load()
  text_splitter = RecursiveCharacterTextSplitter(
      chunk_size = 1000,
      chunk_overlap = 200
  )
  final_documents = text_splitter.split_documents(docs[:50])
  vectors = FAISS.from_documents(final_documents, embeddings)

  # storing
  store["docs"] = docs
  store["embeddings"] = embeddings
  store["final_documents"] = final_documents
  store["vectors"] = vectors

def load_vector_store(running_local = False):
  store = STATE if running_local else st.session_state
  if "vectors" not in store:
    create_vector_embeddings(running_local)

# Function to generate response
def generate_response(user_prompt, api_key, engine, prompt = prompt, running_local = False):
  store = STATE if running_local else st.session_state
  llm = ChatGroq(model = engine,
                 groq_api_key = api_key)
  document_chain = create_stuff_documents_chain(llm, prompt)
  load_vector_store(running_local)

  retriever = store["vectors"].as_retriever()

  if prompt:
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    start = time.process_time()
    response = retrieval_chain.invoke({"input":user_prompt})
    print(f"response time: {time.process_time() - start}")

  if response:
    if running_local:
      print(f"bot: {response['answer']}")
      context_search_required = input("contex_search_required: ").strip().lower()
      if context_search_required in ["yes", "ok", "true"]:
        print("Document Similarity Search")
        for i, doc in enumerate(response['context']):
          print(doc.page_content)
          print("------------")
    else:
      st.write(response['answer'])
      with st.expander("Document similarity serarch"):
        for i, doc in enumerate(response['context']):
          st.write(doc.page_content)


# Question and anwer loop
api_key = specific_key if specific_key else api_key
if running_local:
  while True:
    user_prompt = input("user: ").strip()
    if user_prompt.lower() in ["quit", 'q', 'break', 'exit']:
      break
    if not user_prompt:
      continue
    generate_response(user_prompt = user_prompt,
                      engine = engine,
                      api_key = api_key,
                      running_local = True)
else:
    while True:
        user_prompt = input("user: ").strip()
        if user_prompt.lower() in ["quit", 'q', 'break', 'exit']:
          break
        if not user_prompt:
          continue
        generate_response(
            user_prompt = user_prompt,
            engine = engine,
            api_key= api_key)
